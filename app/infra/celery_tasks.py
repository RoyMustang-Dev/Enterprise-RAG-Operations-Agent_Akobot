"""
Celery Tasks
"""
import os
import asyncio
import logging

from app.infra.celery_app import celery_app
from app.infra.job_tracker import JobTracker

logger = logging.getLogger(__name__)


@celery_app.task(name="ingestion.run_files")
def run_ingestion_files(job_id: str, file_paths: list, metadatas: list, reset_db: bool, tenant_id: str = None):
    """Run file ingestion in a Celery worker."""
    tracker = JobTracker(job_id, {
        "status": "pending",
        "chunks_added": 0,
        "total_chunks": 0,
        "job_id": job_id,
        "logs": ["Celery task started for file ingestion."],
    })
    try:
        from app.ingestion.pipeline import IngestionPipeline
        pipeline = IngestionPipeline(tenant_id=tenant_id)
        pipeline.run_ingestion(
            file_paths=file_paths,
            metadatas=metadatas,
            reset_db=reset_db,
            job_tracker=tracker,
        )
        tracker["status"] = "completed"
        tracker["logs"].append("Celery ingestion completed.")
    except Exception as e:
        tracker["status"] = "failed"
        tracker["error"] = str(e)
        tracker["logs"].append(f"Celery ingestion failed: {e}")
        logger.error(f"[CELERY] File ingestion failed: {e}")


@celery_app.task(name="ingestion.run_crawler")
def run_crawler_job(job_id: str, url: str, max_depth: int, save_folder: str, mode: str, tenant_id: str = None):
    """Run crawler + ingestion in a Celery worker."""
    tracker = JobTracker(job_id, {
        "status": "pending",
        "chunks_added": 0,
        "total_chunks": 0,
        "job_id": job_id,
        "logs": [f"Celery crawler started for URL: {url}"],
    })
    try:
        import sys
        from urllib.parse import urlparse

        if sys.platform == "win32":
            asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())

        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)

        from app.ingestion.crawler_service import CrawlerService
        from app.ingestion.pipeline import IngestionPipeline

        crawler = CrawlerService()
        pipeline = IngestionPipeline(tenant_id=tenant_id)
        reset_db = mode == "overwrite"
        is_first_batch = [True]

        async def process_live_batch(batch_items):
            tracker["status"] = "crawling_and_extracting"
            tracker["logs"].append(f"Streaming {len(batch_items)} scraped pages to Vector Engine...")

            import hashlib

            paths = []
            metas = []
            target_domain = urlparse(url).netloc
            domain_folder = os.path.join(save_folder, target_domain)
            os.makedirs(domain_folder, exist_ok=True)

            for item in batch_items:
                current_url = item[1]
                title = item[2]
                content = item[3]

                safe_name = hashlib.md5(current_url.encode()).hexdigest()
                path = os.path.join(domain_folder, safe_name + ".txt")

                with open(path, "w", encoding="utf-8") as f:
                    f.write(f"== {title} ==\n{content}")

                paths.append(path)
                metas.append(
                    {
                        "type": "url",
                        "source_url": current_url,
                        "source_domain": urlparse(current_url).netloc,
                        "document_type": "webpage",
                    }
                )

            reset = reset_db if is_first_batch[0] else False
            is_first_batch[0] = False

            def ingest_sync():
                pipeline.run_ingestion(
                    paths,
                    metadatas=metas,
                    reset_db=reset,
                    job_tracker=tracker,
                    mark_completed=False,
                )

            await asyncio.to_thread(ingest_sync)

        target_domain = urlparse(url).netloc
        domain_folder = os.path.join(save_folder, target_domain)

        result = loop.run_until_complete(
            crawler.crawl_url(
                url=url,
                save_folder=domain_folder,
                simulate=False,
                recursive=(max_depth > 1),
                max_depth=max_depth,
                on_batch_extracted=process_live_batch,
            )
        )
        loop.close()

        if result.get("saved_files") or not is_first_batch[0]:
            tracker["status"] = "completed"
            tracker["logs"].append("Celery crawler completed.")
        else:
            tracker["status"] = "failed"
            tracker["error"] = "No unstructured text output generated by crawler."
    except Exception as e:
        tracker["status"] = "failed"
        tracker["error"] = str(e)
        tracker["logs"].append(f"Celery crawler failed: {e}")
        logger.error(f"[CELERY] Crawler failed: {e}")
