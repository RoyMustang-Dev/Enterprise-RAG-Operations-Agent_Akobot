"""
Document Retriever Tool Module

Standalone tool responsible for executing Vector Database search operations.
It intercepts the query generated by the orchestrator, converts it into a 
mathematical embedding, and executes semantic mapping against the Vector DB.
"""
import os
import json
from typing import List, Dict, Any
import logging

class RetrieverTool:
    """
    Standalone module responsible for Vector Search (and future Hybrid Search).
    Abstracts direct database operations (FAISS/Qdrant) from the cognitive orchestration layer,
    allowing us to hot-swap database engines without breaking the internal LangGraph logic.
    """
    
    def __init__(self, faiss_store, embedding_model, top_k=20, final_k=5):
        """
        Initializes the retriever tool with active database connections.
        
        Args:
            faiss_store: The active Vector Store adapter instance (FAISS/Qdrant).
            embedding_model: The SentenceTransformer engine used to map queries to vector space.
            top_k (int): Initial broad fetch limit (used to pull enough chunks for filtering/re-ranking).
            final_k (int): Hard limit on chunks returned to the LLM to strictly prevent Token Context Overflow.
        """
        self.faiss_store = faiss_store
        self.embedding_model = embedding_model
        self.top_k = top_k
        self.final_k = final_k
        
    def search(self, query: str) -> List[Dict[str, Any]]:
        """
        Executes semantic search over the vector store.
        
        Args:
            query (str): The search string or rewritten AI prompt.
            
        Returns:
            List[Dict]: An array of dictionaries, each containing raw text and source metadata.
        """
        try:
            logging.info(f"RetrieverTool: Generating embedding for '{query}'")
            # Convert text string to high-dimensional float array
            query_embedding = self.embedding_model.generate_embedding(query)
            
            # -------------------------------------------------------------------------
            # 1. Base Semantic Retrieval
            # -------------------------------------------------------------------------
            initial_results = self.faiss_store.search(query_embedding, k=self.top_k)
            if not initial_results:
                return []
                
            # -------------------------------------------------------------------------
            # 2. Source Filtering (Naive Metadata Matching)
            # -------------------------------------------------------------------------
            # TODO: Phase 8 Upgrade - Replace hardcoded heuristic with LLM JSON extraction
            # Currently a naive implementation checking if target testing filenames exist in the string.
            mentioned_files = [f for f in ["AAICLAS1161721280920.pdf", "Adi_Resume_N.pdf", "content.txt"] if f.lower() in query.lower()]
            filtered_results = initial_results
            
            if mentioned_files:
                logging.info(f"RetrieverTool: Filtering results to explicitly mentioned files: {mentioned_files}")
                # Enforce strict metadata gating if specific files are requested
                filtered_results = [r for r in initial_results if r.get("source", "") in mentioned_files]
                if not filtered_results:
                     logging.warning(f"RetrieverTool: No chunks found in {mentioned_files}, falling back to all.")
                     filtered_results = initial_results
                     
            # -------------------------------------------------------------------------
            # 3. Context Contextual Limiter & Security Stripper
            # -------------------------------------------------------------------------
            # Slice the final array to prevent injecting 20 chunks and blowing up the 8k/16k LLM context limits
            # TODO: Phase 8 Upgrade - Insert Cross-Encoder Reranker here before slicing `[:final_k]`
            final_chunks = filtered_results[:self.final_k]
            
            # Security & Payload Optimization: Strip the massive float arrays out of the dictionaries 
            # before sending them back up the pipeline to keep memory footprints minimal.
            for c in final_chunks:
                if "embedding" in c:
                    del c["embedding"]
                    
            return final_chunks
            
        except Exception as e:
            logging.error(f"RetrieverTool failed during Semantic mapping: {e}")
            return []
